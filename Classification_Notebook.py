
# coding: utf-8

# #  Определение класса аварийности транспортного средства

# По данным телеметрического мониторинга состояния карьерных самосвалов, полученных на предприятиях Coal Inc. в ноябре 2019 года необходимо построить модель определения класса аварийности транспортного средства.

# **Описание данных:**

# Таблица data:  
# - *temperature* — cредняя температура воздуха за бортом;
# - *velocity* — cредняя скорость движения самосвала;
# - *pressure* — cреднее давление в шинах;
# - *incline* — cреднее значение показаний инклинометра;
# - *class* — класс аварийности
# 
# Предложенный массив данных содержит 135 записей с описанными выше измерениями и классом аварийности самосвала, присвоенным ему ремонтной бригадой.

# In[1]:


import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats as st
import math


# In[2]:


data=pd.read_csv('./data/data.csv')


# In[3]:


data.head()


# Многие алгоритмы машинного обучения требуют на вход численные признаки, поэтому приведем метку класса к целочисленному типу.

# In[4]:


data.loc[:,'class']=data['class'].map({'Class_1': 1, 'Class_2': 2, 'Class_3': 3})


# In[5]:


data.info()


# ## Шаг 1. Статистический анализ

# Посмотрим на распределение имеющихся данных по классам. 

# In[6]:


data['class'].value_counts()


# Распределение практически равномерное.

# Обратимся к совместному распределению классов.

# In[7]:


sns.pairplot(data)


# Построим матрицу корреляции, показывающую линейный коэффициент корреляции между всеми возможными парами переменных.

# In[8]:


data.corr()


# Построим так же сводную таблицу средних значений телеметрических показателей для каждого класса аварийности. 

# In[9]:


data.pivot_table(index='class', aggfunc='mean')


# ## Вывод

# Сильная линейная зависимость (с коэффициентром корреляции 0.96) наблюдается классом давлением в шинах и показаниями инклинометра. При превышении наклоном определенного придела давление в шинах значительно повышается. Можно так же заметить, все самосвалы, относящиеся к 1 классу аварийности имели дваление в шинах в пределе 1-2 атм (соответственно работая на в основном в несложных условиях небольших наклонов). Можно сделать вывод, что большая часть аварий, приходится на самосвалы, работающие в более тяжелых условиях. Средний показатель давления в шинах для самосвала отнесенного ко второму классу аварийности - 4.26, для самосвала третьего класса - 5.55. Это хороший показатель для предсказания итогового класса аварийности. С ростом наклона (и соответвующего ему давления в шинах) повышается и класс аварийности. 
# 
# Некоторая линейная зависимость (с коэффициентром корреляции 0.78) наблюдается между классом аварийности и температурой окружающего воздуха. Чем выше температура, тем выше класс аварийности, однако этот показатель не такой силен, при одинаковой температуре встречаются самосвалы всех трех классов аварийности. Ко всему прочему температура - параметр который практически невозможно контролировать. Несмотря на это она может помочь предсказать класс аварийности.
# 
# Зависимость класса опасности от скорости нелинейна. Однако можно заметить, что самосвалы 1 класса как правило имеют большую среднюю скорость (равную 3.44). Как было выяснено ранее, это может быть связано с тем, что самосвалы 1 класса работают в более легких условиях небольшого наклона и способны достичь более высокой скорости в принципе. Однако при сложных условиях наблюдается зависимость между скоростью и аварийностью. Самосвалы 3 класса двигались со средней скоростью 3, что выше среднего показателя скорости у самосвалов второго класса (составляющей 2.8). В данном случае более высокая скорость коррелирует с повышением класса аварийности. 
# 
# **Практические решения:** Наиболее сильно аварийность самосвалов повышается при работе в тяжелых условиях, характеризующихся высокими показаниями инклинометра, ведущими к повышенному давлению в шинах. Для понижения общего уровня аварийности следует своевременно выполненять вертикальную планировку промплощадки, облегчая тем самым условия для ведения работ. В реальных условиях угольного разреза, изобилующего съездами и имеющего высокие темпы ведения горных работ, зачастую невозможно значительно улучшить производственные условия. В этом случае рекомендуется ограничить допустимую скорость движения самосвалов при работе в тяжелых условиях. Это поможет уменьшить число самосвалов, отнесенных к 3 классу аварийности.   
# Так же некоторое улучшение может наступить, если проводить ротацию техники, работающую в различных условиях. Это позволит распределить загруженность ремонтных бригад по времени. 
# 

# ___

# ## Шаг 2. Построение аналитических моделей. 

# Исходные данные содержат небольшое число записей, каждая из которых состоит из 4 численных параметров и 1 метки класса. Классы хорошо различимы даже визуально (например, хорошо видно, что к 1 классу относятся только машины, уровень давления в шинах которых не превышает 2 атм). Для таких данных хорошо подходят простые алгоритмы классификации, вроде k - ближайшах соседей или деревья решений. 

# Обучим несколько алгоритмов и сравним полученные результаты. В качестве моделей будут использованы метод k - ближайших соседей, дерево решений и ансамблевый алгоритм random forest (случайный лес).  
# 
# В работе использована библиотека scikit-learn.

# - Для выполнения контроля за переобучением разобъем имеющийся датасет на тренировочную и тестовую часть. 

# In[10]:


from sklearn.model_selection import train_test_split


# Разделим датасет на две части X, содержащий телеметрические показатели и y, содержащий метки классов. 

# In[11]:


X = data.iloc[:, 0:4].values
y = data.iloc[:, 4].values


# Случайным образом разделим данные на тренировочную и тестовую часть. Размер тестовой части ввиду небольного количества исходных данных примем равным 20% от общего размера датасета.

# In[12]:


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# - Первый исползуемый алгоритм - метод k-ближайших соседей.

# In[13]:


from sklearn.neighbors import KNeighborsClassifier


# В данный момент неизвестно, какие параметры будут наиболее удачными, поэтому примем необходимое число соседей для определения метки класса равным 5. 

# In[14]:


knn = KNeighborsClassifier(n_neighbors=5)


# Обучим модель на тренировочных данных. 

# In[15]:


knn.fit(X_train, y_train)


# Сделаем прогнозы для тестовой выборки. В качестве метрики для определения качества прогноза здесь и далее используем accuracy score - долю правильных ответов. 

# In[52]:


from sklearn.metrics import classification_report, confusion_matrix, accuracy_score


# In[17]:


knn_pred = knn.predict(X_test)


# In[54]:


confusion_matrix(y_test, knn_pred)


# In[53]:


accuracy_score(y_test, knn_pred)


# Как видно даже самым простым и ненастроенным алгоритмом удалось добиться хорошей точности в 96,3%

# Однако мы можем так-же настроить параметры модели с помощью кросс-валидации. В качестве настраиваемог параметра возьмем необходимое число соседей. Для каждого значения в диапазоне от 1 до 10 будет проведена 5-кратная кросс-валидация, и определено наиболее подходящее значение параметра.

# In[19]:


from sklearn.model_selection import GridSearchCV


# In[20]:


knn_params = {'n_neighbors': range(1, 10)}


# In[21]:


knn_new = KNeighborsClassifier(n_neighbors=5)


# In[22]:


knn_grid = GridSearchCV(knn_new, knn_params,
                         cv=5, n_jobs=-1,
                        verbose=True)


# In[23]:


knn_grid.fit(X_train, y_train)


# In[24]:


knn_grid.best_params_, knn_grid.best_score_


# Лучшие результаты модель показывает при параметре n_neighbors равном 7

# In[25]:


knn_grid_pred = knn_grid.predict(X_test)


# In[55]:


confusion_matrix(y_test, knn_grid_pred)


# In[26]:


accuracy_score(y_test, knn_grid_pred)


# Видно, что настройка не повлияла на итоговое качество модели. 

# - Второй исползуемый алгоритм - дерево решений.

# In[27]:


from sklearn.tree import DecisionTreeClassifier


# Удачные параметры дерева нам неизвестны, поэтому зададим параметры по умолчанию, ограничивая при этом максимальную глубину дерева для избежания переобучения.

# In[28]:


tree = DecisionTreeClassifier(random_state=42, max_depth=5)


# In[29]:


tree.fit(X_train, y_train)


# Сделаем прогнозы по тестовой выборке на основе обученой модели. 

# In[30]:


tree_pred = tree.predict(X_test)


# In[56]:


confusion_matrix(y_test, tree_pred)


# In[31]:


accuracy_score(y_test, tree_pred)


# Процент верных предсказаний равен с полученым с помошью метода k-ближайших соседей. 

# Произведем настройку параметров дерева. Максимальная глубина и минимальное число элементов в листе настраивается на 5-кратной кросс-валидации.

# In[32]:


tree_params = {'max_depth': list(range(1, 5)), 
               'min_samples_leaf': list(range(1, 5)),
                'min_samples_split': list(range(1, 5))}


# In[33]:


tree_new = DecisionTreeClassifier(max_depth=5, random_state=42)


# In[34]:


tree_grid = GridSearchCV(tree_new, tree_params,
cv=5, n_jobs=-1,
verbose=True)


# In[35]:


tree_grid.fit(X_train, y_train)


# In[36]:


tree_grid.best_params_, tree_grid.best_score_


# После определения лучших параметров выполним предсказания. 

# In[37]:


tree_grid_pred = tree_grid.predict(X_test)


# In[57]:


confusion_matrix(y_test, tree_grid_pred)


# In[38]:


accuracy_score(y_test, tree_grid_pred)


# Как видно после настройки дерева параметры так же не изменились. 

# - Следующий используемый классификатор - случайный лес. 

# In[39]:


from sklearn.ensemble import RandomForestClassifier


# Случайный лес представляет собой совокупность решающих деревьев. Данные полученые по каждому дереву, входящему в состав леса усредняются. Этот способ должен показывать наибольшую точность. Количество деревьев в составе леса зададим равным 100.

# In[40]:


forest = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)


# In[41]:


forest.fit(X_train, y_train)


# Сделаем прогнозы по тестовой выборке. 

# In[42]:


forest_pred = forest.predict(X_test)


# In[58]:


confusion_matrix(y_test, forest_pred)


# In[43]:


accuracy_score(y_test, forest_pred)


# In[63]:


logit_pred = logit_searcher.predict(X_test)


# In[64]:


accuracy_score(y_test, logit_pred)

